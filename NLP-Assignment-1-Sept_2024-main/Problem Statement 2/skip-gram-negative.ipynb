{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Granth\n",
      "[nltk_data]     Bagadia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(object):\n",
    "    def __init__(self, N, window_size):\n",
    "        self.N = N\n",
    "        self.X_train = []\n",
    "        self.y_train = []\n",
    "        self.window_size = window_size\n",
    "        self.alpha = 0.001\n",
    "        self.words = []\n",
    "        self.word_index = {}\n",
    "\n",
    "    def initialize(self, V, data):\n",
    "        self.V = V\n",
    "        self.W1 = np.random.uniform(-0.8, 0.8, (self.V, self.N))\n",
    "        self.W2 = np.random.uniform(-0.8, 0.8, (self.N, self.V))\n",
    "        self.words = data\n",
    "        for i in range(len(data)):\n",
    "            self.word_index[data[i]] = i\n",
    "\n",
    "    def preprocessing(self, corpus):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        training_data = []\n",
    "        sentences = corpus.split(\".\")\n",
    "        for i in range(len(sentences)):\n",
    "            sentences[i] = sentences[i].strip()\n",
    "            sentence = sentences[i].split()\n",
    "            x = [word.strip(string.punctuation) for word in sentence if word not in stop_words]\n",
    "            x = [word.lower() for word in x]\n",
    "            training_data.append(x)\n",
    "        return training_data\n",
    "\n",
    "    def prepare_data_for_training(self, corpus):\n",
    "        sentences = self.preprocessing(corpus)\n",
    "        data = {}\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in data:\n",
    "                    data[word] = 1\n",
    "                else:\n",
    "                    data[word] += 1\n",
    "\n",
    "        V = len(data)\n",
    "        data = sorted(list(data.keys()))\n",
    "        vocab = {data[i]: i for i in range(len(data))}\n",
    "\n",
    "        for sentence in sentences:\n",
    "            for i in range(len(sentence)):\n",
    "                center_word = [0 for _ in range(V)]\n",
    "                center_word[vocab[sentence[i]]] = 1\n",
    "                context = [0 for _ in range(V)]\n",
    "                for j in range(i - self.window_size, i + self.window_size + 1):\n",
    "                    if i != j and j >= 0 and j < len(sentence):\n",
    "                        context[vocab[sentence[j]]] += 1\n",
    "                self.X_train.append(center_word)\n",
    "                self.y_train.append(context)\n",
    "\n",
    "        self.initialize(V, data)\n",
    "\n",
    "    def feed_forward(self, X):\n",
    "        self.h = np.dot(self.W1.T, X).reshape(self.N, 1)\n",
    "        self.u = np.dot(self.W2.T, self.h)\n",
    "        self.y = self.softmax(self.u)\n",
    "        return self.y\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x))\n",
    "        return exp_x / exp_x.sum(axis=0)\n",
    "\n",
    "    def backpropagate(self, x, t):\n",
    "        e = self.y - np.asarray(t).reshape(self.V, 1)\n",
    "        dLdW2 = np.dot(self.h, e.T)\n",
    "        X = np.array(x).reshape(self.V, 1)\n",
    "        dLdW1 = np.dot(X, np.dot(self.W2, e).T)\n",
    "        self.W2 -= self.alpha * dLdW2\n",
    "        self.W1 -= self.alpha * dLdW1\n",
    "\n",
    "    def get_negative_samples(self, target_word_index, num_samples):\n",
    "        neg_samples = []\n",
    "        while len(neg_samples) < num_samples:\n",
    "            neg_word_index = random.randint(0, self.V - 1)\n",
    "            if neg_word_index != target_word_index:\n",
    "                neg_samples.append(neg_word_index)\n",
    "        return neg_samples\n",
    "\n",
    "    def train(self, epochs, negative_samples):\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            self.loss = 0\n",
    "            for j in range(len(self.X_train)):\n",
    "                center_word_vector = self.X_train[j]\n",
    "                context_vector = self.y_train[j]\n",
    "\n",
    "\n",
    "                self.feed_forward(center_word_vector)\n",
    "                self.backpropagate(center_word_vector, context_vector)\n",
    "\n",
    "\n",
    "                center_word_index = np.argmax(center_word_vector)\n",
    "                neg_samples = self.get_negative_samples(center_word_index, negative_samples)\n",
    "\n",
    "                for neg_index in neg_samples:\n",
    "\n",
    "                    neg_context_vector = [0] * self.V\n",
    "                    neg_context_vector[neg_index] = 1\n",
    "                    self.feed_forward(center_word_vector)\n",
    "                    self.backpropagate(center_word_vector, neg_context_vector)\n",
    "\n",
    "\n",
    "                C = 0\n",
    "                for m in range(self.V):\n",
    "                    if context_vector[m]:\n",
    "                        self.loss += -1 * self.u[m][0]\n",
    "                        C += 1\n",
    "                self.loss += C * np.log(np.sum(np.exp(self.u)))\n",
    "            if epoch % 1000 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {self.loss}\")\n",
    "            self.alpha *= 1 / (1 + self.alpha * epoch)\n",
    "\n",
    "    def predict(self, word, number_of_predictions):\n",
    "        if word in self.words:\n",
    "            index = self.word_index[word]\n",
    "            X = [0 for _ in range(self.V)]\n",
    "            X[index] = 1\n",
    "            prediction = self.feed_forward(X)\n",
    "            output = {}\n",
    "            for i in range(self.V):\n",
    "                output[prediction[i][0]] = i\n",
    "            top_context_words = []\n",
    "            for k in sorted(output, reverse=True):\n",
    "                top_context_words.append(self.words[output[k]])\n",
    "                if len(top_context_words) >= number_of_predictions:\n",
    "                    break\n",
    "            return top_context_words\n",
    "        else:\n",
    "            print(\"Word not found in dictionary\")\n",
    "\n",
    "    def compute_similarity(self, vec1, vec2):\n",
    "        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "    def rank_words(self, target_vector):\n",
    "        similarities = {}\n",
    "        for i in range(self.V):\n",
    "            word_vector = self.W1[i]\n",
    "            similarity = self.compute_similarity(target_vector, word_vector)\n",
    "            similarities[i] = similarity\n",
    "        ranked_words = sorted(similarities, key=similarities.get, reverse=True)\n",
    "        return ranked_words\n",
    "\n",
    "    def compute_mrr_for_window(self, target_word, context_words):\n",
    "        target_index = self.word_index[target_word]\n",
    "        target_vector = self.W1[target_index]\n",
    "\n",
    "        ranked_indices = self.rank_words(target_vector)\n",
    "\n",
    "        mrr = 0\n",
    "        for context_word in context_words:\n",
    "            context_index = self.word_index[context_word]\n",
    "            rank = ranked_indices.index(context_index) + 1\n",
    "            mrr += 1 / rank\n",
    "        return mrr / len(context_words)\n",
    "\n",
    "    def compute_mrr(self, test_data):\n",
    "        total_mrr = 0\n",
    "        for window in test_data:\n",
    "            target_word, context_words = window[0], window[1:]\n",
    "            total_mrr += self.compute_mrr_for_window(target_word, context_words)\n",
    "        average_mrr = total_mrr / len(test_data)\n",
    "        return average_mrr\n",
    "\n",
    "    def evaluate_mrr(self, test_corpus):\n",
    "        test_sentences = self.preprocessing(test_corpus)\n",
    "        test_data = []\n",
    "        for sentence in test_sentences:\n",
    "            for i in range(len(sentence)):\n",
    "                window = sentence[max(0, i - self.window_size):min(len(sentence), i + self.window_size + 1)]\n",
    "                test_data.append(window)\n",
    "\n",
    "        mrr = self.compute_mrr(test_data)\n",
    "        print(\"MRR:\", mrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = \"The earth revolves around the sun. The moon revolves around the earth.\"\n",
    "test_corpus = \"The sun revolves around the earth. The earth revolves around the moon.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000, Loss: 40.201049640150515\n",
      "Epoch 2000, Loss: 40.17765489533684\n",
      "Epoch 3000, Loss: 40.169494359353294\n",
      "Epoch 4000, Loss: 40.165549987689054\n",
      "Epoch 5000, Loss: 40.16328901236755\n",
      "Epoch 6000, Loss: 40.16172556129341\n",
      "Epoch 7000, Loss: 40.160648351432584\n",
      "Epoch 8000, Loss: 40.159855153536824\n",
      "Epoch 9000, Loss: 40.15924611745983\n",
      "Epoch 10000, Loss: 40.15875543000474\n"
     ]
    }
   ],
   "source": [
    "skipGram = SkipGram(N=50, window_size=2)\n",
    "skipGram.prepare_data_for_training(train_corpus)\n",
    "skipGram.train(epochs=10000, negative_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR: 0.3377777777777778\n"
     ]
    }
   ],
   "source": [
    "skipGram.evaluate_mrr(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['earth', 'revolves', 'sun']\n",
      "[ 0.10300137  0.5440801   0.19187793 -0.18084309  0.56858257 -0.81117868\n",
      "  0.86946423  0.31258705 -0.46442859  0.42939062 -0.33877956  0.26585697\n",
      " -0.77899383  0.84464795  0.55410059 -0.26953526  0.0629062  -0.73567159\n",
      " -0.58223677  0.66609512  0.46389084 -0.49299219  0.49264696  0.68486236\n",
      " -0.48251843  0.47207132  0.25314679  0.0023692   0.10809423  0.21426151\n",
      " -0.162643    0.59679787 -0.70144888  0.71300725 -0.58103881  0.43951779\n",
      "  0.86931822  0.13632959  0.17345098 -0.09002836 -0.33356811 -0.42807778\n",
      "  0.28549781 -0.49739613  0.23173142  0.34241522 -0.08334234 -0.32194007\n",
      "  0.49998108  0.76961949]\n"
     ]
    }
   ],
   "source": [
    "print(skipGram.predict(\"around\", 3))\n",
    "print(skipGram.W1[skipGram.word_index[\"around\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

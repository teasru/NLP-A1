{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Granth\n",
      "[nltk_data]     Bagadia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(object):\n",
    "    def __init__(self, N, window_size):\n",
    "        self.N = N\n",
    "        self.X_train = []\n",
    "        self.y_train = []\n",
    "        self.window_size = window_size\n",
    "        self.alpha = 0.001\n",
    "        self.words = []\n",
    "        self.word_index = {}\n",
    "\n",
    "    def initialize(self, V, data):\n",
    "        self.V = V\n",
    "        self.W1 = np.random.uniform(-0.8, 0.8, (self.V, self.N))\n",
    "        self.W2 = np.random.uniform(-0.8, 0.8, (self.N, self.V))\n",
    "        self.words = data\n",
    "        for i in range(len(data)):\n",
    "            self.word_index[data[i]] = i\n",
    "\n",
    "    def preprocessing(self, corpus):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        training_data = []\n",
    "        sentences = corpus.split(\".\")\n",
    "        for i in range(len(sentences)):\n",
    "            sentences[i] = sentences[i].strip()\n",
    "            sentence = sentences[i].split()\n",
    "            x = [word.strip(string.punctuation) for word in sentence if word not in stop_words]\n",
    "            x = [word.lower() for word in x]\n",
    "            training_data.append(x)\n",
    "        return training_data\n",
    "\n",
    "    def prepare_data_for_training(self, corpus):\n",
    "        sentences = self.preprocessing(corpus)\n",
    "        data = {}\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in data:\n",
    "                    data[word] = 1\n",
    "                else:\n",
    "                    data[word] += 1\n",
    "        V = len(data)\n",
    "        data = sorted(list(data.keys()))\n",
    "        vocab = {}\n",
    "        for i in range(len(data)):\n",
    "            vocab[data[i]] = i\n",
    "\n",
    "        for sentence in sentences:\n",
    "            for i in range(len(sentence)):\n",
    "                context = [0 for x in range(V)]\n",
    "                center_word = [0 for x in range(V)]\n",
    "                center_word[vocab[sentence[i]]] = 1\n",
    "                for j in range(i - self.window_size, i + self.window_size + 1):\n",
    "                    if i != j and j >= 0 and j < len(sentence):\n",
    "                        context[vocab[sentence[j]]] += 1\n",
    "                self.X_train.append(context)\n",
    "                self.y_train.append(center_word)\n",
    "\n",
    "        self.initialize(V, data)\n",
    "        return self.X_train, self.y_train\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x))\n",
    "        return exp_x / exp_x.sum(axis=0)\n",
    "\n",
    "    def feed_forward(self, X):\n",
    "        self.h = np.dot(self.W1.T, X).reshape(self.N, 1)\n",
    "        self.u = np.dot(self.W2.T, self.h)\n",
    "        self.y = self.softmax(self.u)\n",
    "        return self.y\n",
    "\n",
    "    def backpropagate(self, x, t):\n",
    "        e = self.y - np.asarray(t).reshape(self.V, 1)\n",
    "        dLdW2 = np.dot(self.h, e.T)\n",
    "        X = np.array(x).reshape(self.V, 1)\n",
    "        dLdW1 = np.dot(X, np.dot(self.W2, e).T)\n",
    "        self.W2 = self.W2 - self.alpha * dLdW2\n",
    "        self.W1 = self.W1 - self.alpha * dLdW1\n",
    "\n",
    "    def train(self, epochs):\n",
    "        for epoch in range(1, epochs):\n",
    "            self.loss = 0\n",
    "            for j in range(len(self.X_train)):\n",
    "                self.feed_forward(self.X_train[j])\n",
    "                self.backpropagate(self.X_train[j], self.y_train[j])\n",
    "                C = 0\n",
    "                for m in range(self.V):\n",
    "                    if self.y_train[j][m]:\n",
    "                        self.loss += -1 * self.u[m][0]\n",
    "                        C += 1\n",
    "                self.loss += C * np.log(np.sum(np.exp(self.u)))\n",
    "            if epoch % 1000 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {self.loss}\")\n",
    "            self.alpha *= 1 / (1 + self.alpha * epoch)\n",
    "\n",
    "    def predict(self, context_words, number_of_predictions):\n",
    "        X = [0 for i in range(self.V)]\n",
    "        for word in context_words:\n",
    "            if word in self.words:\n",
    "                index = self.word_index[word]\n",
    "                X[index] += 1\n",
    "\n",
    "        prediction = self.feed_forward(X)\n",
    "        output = {}\n",
    "        for i in range(self.V):\n",
    "            output[prediction[i][0]] = i\n",
    "        top_predictions = []\n",
    "        for k in sorted(output, reverse=True):\n",
    "            top_predictions.append(self.words[output[k]])\n",
    "            if len(top_predictions) >= number_of_predictions:\n",
    "                break\n",
    "        return top_predictions\n",
    "\n",
    "    def compute_similarity(self, vec1, vec2):\n",
    "        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "    def rank_words(self, target_vector):\n",
    "        similarities = {}\n",
    "        for i in range(self.V):\n",
    "            word_vector = self.W1[i]\n",
    "            similarity = self.compute_similarity(target_vector, word_vector)\n",
    "            similarities[i] = similarity\n",
    "        ranked_words = sorted(similarities, key=similarities.get, reverse=True)\n",
    "        return ranked_words\n",
    "\n",
    "    def compute_mrr_for_window(self, target_word, context_words):\n",
    "        target_index = self.word_index[target_word]\n",
    "        target_vector = self.W1[target_index]\n",
    "\n",
    "        ranked_indices = self.rank_words(target_vector)\n",
    "\n",
    "        mrr = 0\n",
    "        for context_word in context_words:\n",
    "            context_index = self.word_index[context_word]\n",
    "            rank = ranked_indices.index(context_index) + 1\n",
    "            mrr += 1 / rank\n",
    "        return mrr / len(context_words)\n",
    "\n",
    "    def compute_mrr(self, test_data):\n",
    "        total_mrr = 0\n",
    "        for window in test_data:\n",
    "            target_word, context_words = window[0], window[1:]\n",
    "            total_mrr += self.compute_mrr_for_window(target_word, context_words)\n",
    "        average_mrr = total_mrr / len(test_data)\n",
    "        return average_mrr\n",
    "\n",
    "    def evaluate_mrr(self, test_corpus):\n",
    "        test_sentences = self.preprocessing(test_corpus)\n",
    "        test_data = []\n",
    "        for sentence in test_sentences:\n",
    "            for i in range(len(sentence)):\n",
    "                window = sentence[max(0, i - self.window_size):min(len(sentence), i + self.window_size + 1)]\n",
    "                test_data.append(window)\n",
    "\n",
    "        mrr = self.compute_mrr(test_data)\n",
    "        print(\"MRR:\", mrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = \"The earth revolves around the sun. The moon revolves around the earth.\"\n",
    "test_corpus = \"The sun revolves around the earth. The earth revolves around the moon.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000, Loss: 10.351679490353394\n",
      "Epoch 2000, Loss: 10.279755304409866\n",
      "Epoch 3000, Loss: 10.256210184958494\n",
      "Epoch 4000, Loss: 10.244518057426468\n",
      "Epoch 5000, Loss: 10.237528499758572\n",
      "Epoch 6000, Loss: 10.232879505935024\n",
      "Epoch 7000, Loss: 10.229564040792393\n",
      "Epoch 8000, Loss: 10.227080309512221\n",
      "Epoch 9000, Loss: 10.225150217536665\n"
     ]
    }
   ],
   "source": [
    "cbow = CBOW(N=50, window_size=2)\n",
    "cbow.prepare_data_for_training(train_corpus)\n",
    "cbow.train(epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR: 0.2801388888888888\n"
     ]
    }
   ],
   "source": [
    "cbow.evaluate_mrr(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word not found in dictionary\n",
      "None\n",
      "[-0.53256885 -0.70563596  0.07855461  0.40833322 -0.30250257  0.51874767\n",
      "  0.44018612  0.51278246 -0.13371677  0.58826988  0.45678772 -0.35019537\n",
      " -0.37782976  0.108365    0.61816973  0.04530163 -0.70084048 -0.48972064\n",
      " -0.36178548 -0.23688827 -0.17547803 -0.15102303 -0.54871521 -0.44878585\n",
      "  0.75011751 -0.52981644  0.50228706 -0.71437914  0.10798189 -0.04767139\n",
      "  0.27967491  0.56509143 -0.08984134 -0.19054663  0.09598199 -0.18236667\n",
      " -0.56642973  0.21170609 -0.38328395 -0.19801797  0.63954692 -0.42971867\n",
      " -0.51023874  0.42693253 -0.27577159 -0.23057178  0.53139705 -0.02095248\n",
      "  0.37192606 -0.68750596]\n"
     ]
    }
   ],
   "source": [
    "print(cbow.predict([\"the\", \"revolves\", \"sun\"], 3))\n",
    "print(cbow.W1[cbow.word_index[\"earth\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
